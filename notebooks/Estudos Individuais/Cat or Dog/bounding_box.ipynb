{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\breno\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as F\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatDogClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CatDogClassifier, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16 * 16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, 1)  # Saída como logits\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x  # Saída como logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CatDogClassifier(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU()\n",
       "    (10): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=32768, out_features=128, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Dropout(p=0.5, inplace=False)\n",
       "    (7): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = CatDogClassifier().to(args['device'])\n",
    "net.load_state_dict(torch.load('cat_dog_classifier.pth', map_location=torch.device(args['device'])))\n",
    "net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_image(model, image_path, device):\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)  \n",
    "\n",
    "    image = image.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Faz a inferência\n",
    "        output = model(image)\n",
    "        probability = torch.sigmoid(output).item() \n",
    "\n",
    "    print(f'Probabilidade: {probability}')\n",
    "    if probability > 0.5:\n",
    "        return \"Cachorro\"\n",
    "    else:\n",
    "        return \"Gato\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidade: 1.2716908486254397e-06\n",
      "Gato\n"
     ]
    }
   ],
   "source": [
    "inference = infer_image(net, 'inferences/gato.jpg', args['device'])\n",
    "print(inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_with_faster_rcnn_and_classify(image_path, faster_rcnn, classifier, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Combina Faster R-CNN para detecção de objetos e um modelo de classificação para rotular gatos/cachorros.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Caminho para a imagem.\n",
    "        faster_rcnn (nn.Module): Modelo Faster R-CNN pré-treinado.\n",
    "        classifier (nn.Module): Modelo de classificação treinado.\n",
    "        device (torch.device): Dispositivo (CPU/GPU).\n",
    "        threshold (float): Limiar de confiança para detecção.\n",
    "    \n",
    "    Returns:\n",
    "        PIL.Image: Imagem com bounding boxes e classificações desenhadas.\n",
    "    \"\"\"\n",
    "    # Carregar a imagem\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = image.copy()\n",
    "    draw = ImageDraw.Draw(original_image)\n",
    "\n",
    "    # Transformação usada pelo Faster R-CNN\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    image_tensor = transform(image).to(device)\n",
    "\n",
    "    # Detectar regiões de interesse com Faster R-CNN\n",
    "    faster_rcnn.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = faster_rcnn([image_tensor])\n",
    "\n",
    "    boxes = predictions[0]['boxes']\n",
    "    scores = predictions[0]['scores']\n",
    "\n",
    "    # Transformação usada pelo classificador\n",
    "    classifier_transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Para cada bounding box detectada\n",
    "    for box, score in zip(boxes, scores):\n",
    "        if score > threshold:\n",
    "            # Recortar a região da imagem original\n",
    "            x1, y1, x2, y2 = map(int, box.tolist())\n",
    "            cropped_image = image.crop((x1, y1, x2, y2))\n",
    "\n",
    "            # Pré-processar a região recortada\n",
    "            cropped_tensor = classifier_transform(cropped_image).unsqueeze(0).to(device)\n",
    "\n",
    "            # Classificar a região com o modelo de classificação\n",
    "            classifier.eval()\n",
    "            with torch.no_grad():\n",
    "                output = classifier(cropped_tensor)\n",
    "                probability = torch.sigmoid(output).item()\n",
    "                label = \"Cachorro\" if probability > 0.5 else \"Gato\"\n",
    "\n",
    "            # Desenhar a bounding box e a classificação na imagem original\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=2)\n",
    "            draw.text((x1, y1), f\"{label}\", fill=\"red\")\n",
    "    \n",
    "    return original_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\breno\\OneDrive\\Documentos\\GitHub\\study-deeplearning\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\breno\\OneDrive\\Documentos\\GitHub\\study-deeplearning\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "image_path = r\"inferences\\humano_com_gato2.jpg\"\n",
    "image_path_2 = r'test_set/test_set/cats/cat.4010.jpg'\n",
    "faster_rcnn = fasterrcnn_resnet50_fpn(pretrained=True).to(args['device'])\n",
    "result = detect_with_faster_rcnn_and_classify(image_path, faster_rcnn, net, args['device'])\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_with_yolo_and_classify(image_path, yolo_model_path, classifier_model, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Usa YOLO para detectar bounding boxes e um modelo de classificação para rotular gatos/cachorros.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Caminho para a imagem.\n",
    "        yolo_model_path (str): Caminho para o modelo YOLO pré-treinado ou personalizado.\n",
    "        classifier_model (torch.nn.Module): Modelo de classificação treinado.\n",
    "        device (torch.device): Dispositivo (CPU/GPU).\n",
    "        threshold (float): Limiar de confiança para detecção.\n",
    "\n",
    "    Returns:\n",
    "        PIL.Image: Imagem com bounding boxes desenhadas e classificações.\n",
    "    \"\"\"\n",
    "    # Carregar o modelo YOLO\n",
    "    yolo_model = YOLO(yolo_model_path)\n",
    "\n",
    "    # Fazer a detecção com YOLO\n",
    "    results = yolo_model(image_path)\n",
    "\n",
    "    # Carregar a imagem original\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Transformação usada pelo classificador\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Iterar pelos resultados de detecção\n",
    "    for result in results[0].boxes.data:\n",
    "        x1, y1, x2, y2, confidence, class_id = result.tolist()\n",
    "        if confidence > threshold:\n",
    "            # Recortar a região da imagem original\n",
    "            cropped_image = image.crop((x1, y1, x2, y2))\n",
    "\n",
    "            # Pré-processar a região recortada\n",
    "            cropped_tensor = transform(cropped_image).unsqueeze(0).to(device)\n",
    "\n",
    "            # Classificar a região com o modelo de classificação\n",
    "            classifier_model.eval()\n",
    "            with torch.no_grad():\n",
    "                output = classifier_model(cropped_tensor)\n",
    "                probability = torch.sigmoid(output).item()\n",
    "                label = \"Cachorro\" if probability > 0.5 else \"Gato\"\n",
    "\n",
    "            # Desenhar bounding box e a classe\n",
    "            draw.rectangle([x1, y1, x2, y2], outline=\"red\", width=3)\n",
    "            draw.text((x1, y1), f\"{label}\", fill=\"red\")\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\Users\\breno\\OneDrive\\Documentos\\GitHub\\study-deeplearning\\notebooks\\Estudos Individuais\\Cat or Dog\\inferences\\humano_com_gato.jpg: 384x640 2 persons, 1 cat, 1 couch, 0.0ms\n",
      "Speed: 0.0ms preprocess, 0.0ms inference, 0.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "image_path = r\"inferences\\humano_com_gato.jpg\"\n",
    "result = detect_with_yolo_and_classify(image_path, 'yolov8n.pt', net, args['device'])\n",
    "result.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
